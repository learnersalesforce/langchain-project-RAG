{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wikipedia tool\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "## predefine tool (inbuilt tool)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "# Load Ollama embeddings \n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")  # Change to \"llama2\" if needed\n",
    "\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "## created custom tool\n",
    "retrieval_tool=create_retriever_tool(retriever,\"Langsmith_search\",\"Search for information about langsmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith_search'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv Tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retrieval_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\Updated-Langchain\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='Langsmith_search', description='Search for information about langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000029C144422A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000029C147613A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' The capital of France is Paris.' additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-03-05T10:13:17.6743007Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6710069500, 'load_duration': 2763679200, 'prompt_eval_count': 12, 'prompt_eval_duration': 2263091000, 'eval_count': 8, 'eval_duration': 1675984000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1e7448df-fbc5-4045-a5a5-80450097db6e-0' usage_metadata={'input_tokens': 12, 'output_tokens': 8, 'total_tokens': 20}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_ollama import ChatOllama  # Import open-source LLM\n",
    "\n",
    "# Use an open-source model like \"mistral\" or \"llama2\"\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# Test the model\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" LangChain is a machine learning-powered translation service that allows developers to integrate real-time, high-quality translation of multiple languages into their applications or services. It's designed for seamless communication between different language speakers by accurately translating text and maintaining the context and tone of the original message. LangChain uses advanced Neural Machine Translation (NMT) technology to deliver fast, reliable, and contextually accurate translations. It supports a wide range of languages and can be customized based on specific industry needs for better performance.\" additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-03-05T11:12:39.2658772Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32890027500, 'load_duration': 2467014400, 'prompt_eval_count': 22, 'prompt_eval_duration': 4200999000, 'eval_count': 109, 'eval_duration': 26217852000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-8f3e58b3-acae-475c-aad1-18a8a228c057-0' usage_metadata={'input_tokens': 22, 'output_tokens': 109, 'total_tokens': 131}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Use an open-source model\n",
    "llm = ChatOllama(model=\"mistral\")  # Replace with \"llama2\", \"gemma\", etc.\n",
    "\n",
    "# Format the prompt correctly\n",
    "formatted_prompt = prompt.format(input=\"Explain LangChain\")\n",
    "\n",
    "# Run the model with the correctly formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m LangChain:\n",
      "\n",
      "LangChain is a decentralized machine learning platform built on blockchain technology. It allows developers to create, train, and deploy AI models without relying on centralized cloud services. Instead, the data and computational resources are distributed across a network of nodes, which participate in the training process and earn rewards for their contributions.\n",
      "\n",
      "The key features of LangChain include:\n",
      "\n",
      "1. Decentralization: The platform is built on blockchain technology to ensure decentralization and security.\n",
      "2. Scalability: The architecture is designed to scale horizontally, allowing the platform to handle a large number of users and training tasks.\n",
      "3. Privacy: Since data is distributed across the network, it ensures that user data remains private and secure.\n",
      "4. Interoperability: LangChain supports various programming languages and AI frameworks, making it easy for developers to integrate their existing tools into the platform.\n",
      "5. Incentives: The platform uses a token-based incentive system to reward participants for their contributions.\n",
      "6. Governance: A decentralized autonomous organization (DAO) governs the platform, allowing stakeholders to participate in decision-making processes and vote on proposals.\n",
      "\n",
      "Process 'AI' with LangChain:\n",
      "\n",
      "To process AI tasks using LangChain, developers would follow these steps:\n",
      "\n",
      "1. Define the problem: The first step is to clearly define the problem that needs to be solved using AI. This could be anything from image recognition to natural language processing.\n",
      "2. Choose a framework: Next, choose an appropriate AI framework for the task at hand. LangChain supports various popular frameworks such as TensorFlow, PyTorch, and Keras.\n",
      "3. Prepare data: Gather and preprocess the required data for training the model. This could involve cleaning and labeling the data or obtaining it from publicly available sources.\n",
      "4. Train the model: Use LangChain to train the AI model on the distributed network. The platform automatically selects nodes with suitable computational resources to perform the task.\n",
      "5. Validate and fine-tune: Once the model is trained, validate its performance by testing it on a separate dataset. If necessary, fine-tune the model to improve its accuracy.\n",
      "6. Deploy the model: Once satisfied with the model's performance, deploy it for real-world use cases. LangChain allows models to be deployed as APIs or embedded in other applications.\n",
      "7. Monitor and maintain: Continuously monitor the model's performance and update it as necessary to keep up with changing data patterns.\n",
      "\n",
      "By using LangChain, developers can leverage the power of AI without relying on centralized cloud services, ensuring greater control over their data and computational resources while reducing costs.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"Explain LangChain and process 'AI' with the tool.\", 'output': \" LangChain:\\n\\nLangChain is a decentralized machine learning platform built on blockchain technology. It allows developers to create, train, and deploy AI models without relying on centralized cloud services. Instead, the data and computational resources are distributed across a network of nodes, which participate in the training process and earn rewards for their contributions.\\n\\nThe key features of LangChain include:\\n\\n1. Decentralization: The platform is built on blockchain technology to ensure decentralization and security.\\n2. Scalability: The architecture is designed to scale horizontally, allowing the platform to handle a large number of users and training tasks.\\n3. Privacy: Since data is distributed across the network, it ensures that user data remains private and secure.\\n4. Interoperability: LangChain supports various programming languages and AI frameworks, making it easy for developers to integrate their existing tools into the platform.\\n5. Incentives: The platform uses a token-based incentive system to reward participants for their contributions.\\n6. Governance: A decentralized autonomous organization (DAO) governs the platform, allowing stakeholders to participate in decision-making processes and vote on proposals.\\n\\nProcess 'AI' with LangChain:\\n\\nTo process AI tasks using LangChain, developers would follow these steps:\\n\\n1. Define the problem: The first step is to clearly define the problem that needs to be solved using AI. This could be anything from image recognition to natural language processing.\\n2. Choose a framework: Next, choose an appropriate AI framework for the task at hand. LangChain supports various popular frameworks such as TensorFlow, PyTorch, and Keras.\\n3. Prepare data: Gather and preprocess the required data for training the model. This could involve cleaning and labeling the data or obtaining it from publicly available sources.\\n4. Train the model: Use LangChain to train the AI model on the distributed network. The platform automatically selects nodes with suitable computational resources to perform the task.\\n5. Validate and fine-tune: Once the model is trained, validate its performance by testing it on a separate dataset. If necessary, fine-tune the model to improve its accuracy.\\n6. Deploy the model: Once satisfied with the model's performance, deploy it for real-world use cases. LangChain allows models to be deployed as APIs or embedded in other applications.\\n7. Monitor and maintain: Continuously monitor the model's performance and update it as necessary to keep up with changing data patterns.\\n\\nBy using LangChain, developers can leverage the power of AI without relying on centralized cloud services, ensuring greater control over their data and computational resources while reducing costs.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "\n",
    "# ✅ Define a valid prompt template with `agent_scratchpad`\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{agent_scratchpad}\")  # ✅ Required for agent to work\n",
    "])\n",
    "\n",
    "# ✅ Use an open-source model like Mistral\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# ✅ Define a sample tool\n",
    "def sample_tool_function(query: str) -> str:\n",
    "    return f\"Tool processed query: {query}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Sample Tool\",\n",
    "        func=sample_tool_function,\n",
    "        description=\"A tool that processes queries.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# ✅ Create agent with the correct prompt\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# ✅ Wrap the agent in an AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# ✅ Run the agent with an input\n",
    "response = agent_executor.invoke({\"input\": \"Explain LangChain and process 'AI' with the tool.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m1605.08386, also known as \"Generating Diverse Coherent Text with Recurrent Neural Networks,\" is a research paper published on arXiv in May 2016. The primary focus of the paper is on text generation using recurrent neural networks (RNNs).\n",
      "\n",
      "The authors, Dzmitry Bauza and KyungHyun Cho from Microsoft Research, propose two approaches to generate diverse, coherent text sequences:\n",
      "\n",
      "1. A novel objective function that encourages diversity in generated text by penalizing sentences that are too similar to previously generated ones. This objective function is combined with a regularization term that rewards the model for generating longer and more informative sentences.\n",
      "\n",
      "2. A new sampling technique called Top-k sampling, which selects the most likely words from the top k candidates at each time step during text generation. By randomly sampling among the top k options, the model can produce diverse text sequences while still maintaining a level of coherence.\n",
      "\n",
      "The paper provides empirical evidence that both approaches lead to improved performance in generating diverse and coherent text compared to traditional RNN-based text generation methods. The research has applications in various areas, such as machine translation, summarization, and dialogue systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"What's the paper 1605.08386 about?\", 'output': '1605.08386, also known as \"Generating Diverse Coherent Text with Recurrent Neural Networks,\" is a research paper published on arXiv in May 2016. The primary focus of the paper is on text generation using recurrent neural networks (RNNs).\\n\\nThe authors, Dzmitry Bauza and KyungHyun Cho from Microsoft Research, propose two approaches to generate diverse, coherent text sequences:\\n\\n1. A novel objective function that encourages diversity in generated text by penalizing sentences that are too similar to previously generated ones. This objective function is combined with a regularization term that rewards the model for generating longer and more informative sentences.\\n\\n2. A new sampling technique called Top-k sampling, which selects the most likely words from the top k candidates at each time step during text generation. By randomly sampling among the top k options, the model can produce diverse text sequences while still maintaining a level of coherence.\\n\\nThe paper provides empirical evidence that both approaches lead to improved performance in generating diverse and coherent text compared to traditional RNN-based text generation methods. The research has applications in various areas, such as machine translation, summarization, and dialogue systems.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I apologize for the confusion. The question asked about a cricket match between India and Australia that took place last night, but as of now, there is no scheduled Test or T20 series between these two teams this year, so it would be impossible to determine who won a match that did not occur. This question seems to have been made in error.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'who was win last night in cricket match in between india and austrilia?', 'output': ' I apologize for the confusion. The question asked about a cricket match between India and Australia that took place last night, but as of now, there is no scheduled Test or T20 series between these two teams this year, so it would be impossible to determine who won a match that did not occur. This question seems to have been made in error.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"who was win last night cricket match  between india and austrilia?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
