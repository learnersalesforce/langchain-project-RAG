{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wikipedia tool\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia\n"
     ]
    }
   ],
   "source": [
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=200)\n",
    "## predefine tool (inbuilt tool)\n",
    "wiki=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(wiki.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs=loader.load()\n",
    "documents=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\n",
    "\n",
    "# Load Ollama embeddings \n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")  # Change to \"llama2\" if needed\n",
    "\n",
    "vectordb=FAISS.from_documents(documents,embeddings)\n",
    "retriever=vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "## created custom tool\n",
    "retrieval_tool=create_retriever_tool(retriever,\"Langsmith_search\",\"Search for information about langsmith\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langsmith_search'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Arxiv Tool\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[wiki,arxiv,retrieval_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'd:\\\\Updated-Langchain\\\\venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='Langsmith_search', description='Search for information about langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000029C144422A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000029C147613A0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000029C231A3AD0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' The capital of France is Paris.' additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-03-05T10:13:17.6743007Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6710069500, 'load_duration': 2763679200, 'prompt_eval_count': 12, 'prompt_eval_duration': 2263091000, 'eval_count': 8, 'eval_duration': 1675984000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-1e7448df-fbc5-4045-a5a5-80450097db6e-0' usage_metadata={'input_tokens': 12, 'output_tokens': 8, 'total_tokens': 20}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_ollama import ChatOllama  # Import open-source LLM\n",
    "\n",
    "# Use an open-source model like \"mistral\" or \"llama2\"\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0)\n",
    "\n",
    "# Test the model\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\" LangChain is a machine learning-powered translation service that allows developers to integrate real-time, high-quality translation of multiple languages into their applications or services. It's designed for seamless communication between different language speakers by accurately translating text and maintaining the context and tone of the original message. LangChain uses advanced Neural Machine Translation (NMT) technology to deliver fast, reliable, and contextually accurate translations. It supports a wide range of languages and can be customized based on specific industry needs for better performance.\" additional_kwargs={} response_metadata={'model': 'mistral', 'created_at': '2025-03-05T11:12:39.2658772Z', 'done': True, 'done_reason': 'stop', 'total_duration': 32890027500, 'load_duration': 2467014400, 'prompt_eval_count': 22, 'prompt_eval_duration': 4200999000, 'eval_count': 109, 'eval_duration': 26217852000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-8f3e58b3-acae-475c-aad1-18a8a228c057-0' usage_metadata={'input_tokens': 22, 'output_tokens': 109, 'total_tokens': 131}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Use an open-source model\n",
    "llm = ChatOllama(model=\"mistral\")  # Replace with \"llama2\", \"gemma\", etc.\n",
    "\n",
    "# Format the prompt correctly\n",
    "formatted_prompt = prompt.format(input=\"Explain LangChain\")\n",
    "\n",
    "# Run the model with the correctly formatted prompt\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m LangChain:\n",
      "\n",
      "LangChain is a decentralized, open-source platform built on top of the Ethereum blockchain that aims to create an interoperable ecosystem for building and deploying language models. It allows developers to build, train, and share custom language models in a secure and privacy-preserving manner while incentivizing users to contribute their computing resources through a token economy. LangChain also offers tools and APIs to integrate these models into various applications such as chatbots, translation services, and content generation tools.\n",
      "\n",
      "Process 'AI' with the tool:\n",
      "\n",
      "To use LangChain for processing AI tasks, you can follow these general steps:\n",
      "\n",
      "1. Setup a development environment: Install the necessary dependencies (such as Solidity, Truffle, and web3.js) to work with the Ethereum blockchain and start writing smart contracts in Solidity.\n",
      "\n",
      "2. Define your language model: Write the code for your custom language model using a deep learning framework such as TensorFlow or PyTorch. You can use pre-trained models from popular AI platforms like Hugging Face, or train your own models based on your specific requirements.\n",
      "\n",
      "3. Integrate with LangChain: Deploy your language model to the LangChain platform by creating a smart contract that defines its structure, input/output parameters, and associated data structures. This smart contract will also handle the token economy and resource allocation within the ecosystem.\n",
      "\n",
      "4. Train and optimize your model: Once deployed, you can train and optimize your model using the available computing resources from other users in the LangChain network. You can do this by submitting tasks to the network and rewarding contributors based on their performance and resource contribution.\n",
      "\n",
      "5. Integrate with applications: Use the LangChain APIs to integrate your language model into various applications, such as chatbots, translation services, or content generation tools. This allows you to monetize your AI capabilities and build a diverse range of applications for different industries and use cases.\n",
      "\n",
      "6. Deploy and maintain: After deploying your application, you can continue to monitor its performance, update the underlying language model, and engage with other developers in the LangChain community to improve your solution and contribute back to the ecosystem.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"Explain LangChain and process 'AI' with the tool.\", 'output': \" LangChain:\\n\\nLangChain is a decentralized, open-source platform built on top of the Ethereum blockchain that aims to create an interoperable ecosystem for building and deploying language models. It allows developers to build, train, and share custom language models in a secure and privacy-preserving manner while incentivizing users to contribute their computing resources through a token economy. LangChain also offers tools and APIs to integrate these models into various applications such as chatbots, translation services, and content generation tools.\\n\\nProcess 'AI' with the tool:\\n\\nTo use LangChain for processing AI tasks, you can follow these general steps:\\n\\n1. Setup a development environment: Install the necessary dependencies (such as Solidity, Truffle, and web3.js) to work with the Ethereum blockchain and start writing smart contracts in Solidity.\\n\\n2. Define your language model: Write the code for your custom language model using a deep learning framework such as TensorFlow or PyTorch. You can use pre-trained models from popular AI platforms like Hugging Face, or train your own models based on your specific requirements.\\n\\n3. Integrate with LangChain: Deploy your language model to the LangChain platform by creating a smart contract that defines its structure, input/output parameters, and associated data structures. This smart contract will also handle the token economy and resource allocation within the ecosystem.\\n\\n4. Train and optimize your model: Once deployed, you can train and optimize your model using the available computing resources from other users in the LangChain network. You can do this by submitting tasks to the network and rewarding contributors based on their performance and resource contribution.\\n\\n5. Integrate with applications: Use the LangChain APIs to integrate your language model into various applications, such as chatbots, translation services, or content generation tools. This allows you to monetize your AI capabilities and build a diverse range of applications for different industries and use cases.\\n\\n6. Deploy and maintain: After deploying your application, you can continue to monitor its performance, update the underlying language model, and engage with other developers in the LangChain community to improve your solution and contribute back to the ecosystem.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import create_openai_tools_agent, AgentExecutor\n",
    "\n",
    "# ✅ Define a valid prompt template with `agent_scratchpad`\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{agent_scratchpad}\")  # ✅ Required for agent to work\n",
    "])\n",
    "\n",
    "# ✅ Use an open-source model like Mistral\n",
    "llm = ChatOllama(model=\"mistral\")\n",
    "\n",
    "# ✅ Define a sample tool\n",
    "def sample_tool_function(query: str) -> str:\n",
    "    return f\"Tool processed query: {query}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Sample Tool\",\n",
    "        func=sample_tool_function,\n",
    "        description=\"A tool that processes queries.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# ✅ Create agent with the correct prompt\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "# ✅ Wrap the agent in an AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# ✅ Run the agent with an input\n",
    "response = agent_executor.invoke({\"input\": \"Explain LangChain and process 'AI' with the tool.\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m1605.08386 refers to a research paper titled \"Generating Diverse and Coherent Sentences from a Unified Perspective\" authored by Yonghui Wu, Mirella Lapata, and Alan W. Blackwell. The main objective of this paper is to address the problem of textual diversity in machine-generated texts while maintaining coherence and meaning.\n",
      "\n",
      "The authors propose a novel approach called the \"Unified Text Generation Framework\" (UTF). UTF models text generation as a sequence prediction task, considering each token as an action taken by an agent within a specific environment. The environment is defined using three components: context, dialogue history, and the current utterance.\n",
      "\n",
      "To generate diverse and coherent sentences, UTF adopts a reinforcement learning-based approach to optimize the reward function for generating diverse and coherent text. Specifically, the authors introduce two types of rewards: diversity reward and coherence reward. The diversity reward encourages the model to choose less common words or phrases when generating responses, while the coherence reward ensures that the generated text remains relevant to the dialogue context.\n",
      "\n",
      "The authors evaluate their proposed method on three different datasets and show that it outperforms several baselines in terms of generating diverse and coherent sentences without compromising the overall quality of the output. The paper contributes to the advancement of machine learning techniques for improving the quality and diversity of text generation systems.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"What's the paper 1605.08386 about?\", 'output': '1605.08386 refers to a research paper titled \"Generating Diverse and Coherent Sentences from a Unified Perspective\" authored by Yonghui Wu, Mirella Lapata, and Alan W. Blackwell. The main objective of this paper is to address the problem of textual diversity in machine-generated texts while maintaining coherence and meaning.\\n\\nThe authors propose a novel approach called the \"Unified Text Generation Framework\" (UTF). UTF models text generation as a sequence prediction task, considering each token as an action taken by an agent within a specific environment. The environment is defined using three components: context, dialogue history, and the current utterance.\\n\\nTo generate diverse and coherent sentences, UTF adopts a reinforcement learning-based approach to optimize the reward function for generating diverse and coherent text. Specifically, the authors introduce two types of rewards: diversity reward and coherence reward. The diversity reward encourages the model to choose less common words or phrases when generating responses, while the coherence reward ensures that the generated text remains relevant to the dialogue context.\\n\\nThe authors evaluate their proposed method on three different datasets and show that it outperforms several baselines in terms of generating diverse and coherent sentences without compromising the overall quality of the output. The paper contributes to the advancement of machine learning techniques for improving the quality and diversity of text generation systems.'}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"What's the paper 1605.08386 about?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m2+2=4\n",
      "\n",
      "Now, let's try to find the sum using a visual representation. We can use blocks or dots to represent each number:\n",
      "\n",
      "|   |   |\n",
      "|---|---|\n",
      "|  \\_  |  \\_  | (Each line represents one unit)\n",
      "| /\\  | /\\  |\n",
      "| | |\n",
      "| \\_/ | \\_/ |\n",
      "\n",
      "For the first number, 2, we have 2 lines and 2 diagonal slashes:\n",
      "```markdown\n",
      "     _\n",
      "   /\\_\n",
      "  /    \\\n",
      " /      \\\n",
      "\\_/      \\_\n",
      "```\n",
      "For the second number, 2, it's similar:\n",
      "```markdown\n",
      "     _\n",
      "   /\\_\n",
      "  /    \\\n",
      " /      \\\n",
      "\\_/      \\_\n",
      "```\n",
      "When we combine both representations side by side and add them up vertically, we get:\n",
      "\n",
      "```markdown\n",
      " ___________\n",
      "|           |\n",
      "|   _   _   |\n",
      "|  /\\_/\\_/\\_|\n",
      "| /       \\ |\n",
      "|/         \\|\n",
      "| \\       / |\n",
      "|  \\_/\\_/\\_/ |\n",
      "|           |\n",
      "|           |\n",
      " ___________\n",
      "```\n",
      "Now we have one vertical line, which represents a single unit (the fourth row), and two horizontal lines at the bottom. This means there are two more units hidden beneath those horizontal lines. To find the total sum, we can count all visible and hidden units:\n",
      "\n",
      "1. The fourth row has 1 unit.\n",
      "2. The two horizontal lines each hide one more unit, making a total of 2 more units.\n",
      "3. Therefore, the sum is 1 + 2 = 3. However, since there was a mistake in the original equation (the correct operation was addition instead of multiplication), we should revise our answer to be 4:\n",
      "\n",
      "|   |   |\n",
      "|---|---|\n",
      "| \\_  |  \\_  |\n",
      "| /\\  | /\\  |\n",
      "| | |\n",
      "| \\_/ | \\_/ |\n",
      "\n",
      "Now, let's use a calculator to check our answer:\n",
      "\n",
      "```\n",
      "Input: 2 + 2\n",
      "Result: 4 (Correct!)\n",
      "```\n",
      "Great! We can see that 2+2 equals 4 when using either addition or visual representation. However, it is essential to carefully read and understand the problem given to us before starting to solve it.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '2+2=?', 'output': \"2+2=4\\n\\nNow, let's try to find the sum using a visual representation. We can use blocks or dots to represent each number:\\n\\n|   |   |\\n|---|---|\\n|  \\\\_  |  \\\\_  | (Each line represents one unit)\\n| /\\\\  | /\\\\  |\\n| | |\\n| \\\\_/ | \\\\_/ |\\n\\nFor the first number, 2, we have 2 lines and 2 diagonal slashes:\\n```markdown\\n     _\\n   /\\\\_\\n  /    \\\\\\n /      \\\\\\n\\\\_/      \\\\_\\n```\\nFor the second number, 2, it's similar:\\n```markdown\\n     _\\n   /\\\\_\\n  /    \\\\\\n /      \\\\\\n\\\\_/      \\\\_\\n```\\nWhen we combine both representations side by side and add them up vertically, we get:\\n\\n```markdown\\n ___________\\n|           |\\n|   _   _   |\\n|  /\\\\_/\\\\_/\\\\_|\\n| /       \\\\ |\\n|/         \\\\|\\n| \\\\       / |\\n|  \\\\_/\\\\_/\\\\_/ |\\n|           |\\n|           |\\n ___________\\n```\\nNow we have one vertical line, which represents a single unit (the fourth row), and two horizontal lines at the bottom. This means there are two more units hidden beneath those horizontal lines. To find the total sum, we can count all visible and hidden units:\\n\\n1. The fourth row has 1 unit.\\n2. The two horizontal lines each hide one more unit, making a total of 2 more units.\\n3. Therefore, the sum is 1 + 2 = 3. However, since there was a mistake in the original equation (the correct operation was addition instead of multiplication), we should revise our answer to be 4:\\n\\n|   |   |\\n|---|---|\\n| \\\\_  |  \\\\_  |\\n| /\\\\  | /\\\\  |\\n| | |\\n| \\\\_/ | \\\\_/ |\\n\\nNow, let's use a calculator to check our answer:\\n\\n```\\nInput: 2 + 2\\nResult: 4 (Correct!)\\n```\\nGreat! We can see that 2+2 equals 4 when using either addition or visual representation. However, it is essential to carefully read and understand the problem given to us before starting to solve it.\"}\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke({\"input\": \"2+2=?\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
